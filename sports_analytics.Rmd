---
title: "Big Ideas in Sports Analytics and Statistical Tools for their Investigation"
authors:
  - name: Benjamin S. Baumer
#    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    department: Statistical & Data Sciences
    affiliation: Smith College
    location: Northampton, MA 01063
    email: bbaumer@smith.edu
    ORCID: 0000-0002-3279-0516
  - name: Gregory J. Matthews
    department: Mathematics and Statistics
    affiliation: Loyola University Chicago
    location: Chicago, IL 60660
    email: gmatthews1@luc.edu
    ORCID: 0000-0002-8413-5097
  - name: Quang Nguyen
    department: Statistics & Data Science
    affiliation: Carnegie Mellon University
    location: Pittsburgh, PA 15213
    email: nmquang@cmu.edu
abstract: |
  Sports analytics---broadly defined as the pursuit of improvement in athletic performance through the analysis of data---has expanded its footprint both in the professional sports industry and in academia over the past 30 years. We connect four(?) big ideas that are common across multiple sports, and explore both the shared similarities and individual idiosyncracies of analytical approaches in each sport. While our focus is on the concepts underlying each type of analysis, any implementation necessarily involves both statistical methodologies and sources of data. Where appropriate, we outline how data, models, and knowledge of the sport combine to generate actionable insights. This paper should serve as a useful overview for anyone becoming interested in the study of sports analytics. 
keywords:
  - sports analytics
  - R packages
  - sports data
  - pairwise comparisons
  - datasets
bibliography: [refs.bib, pkgs.bib]
biblio-style: unsrt   
# use apa for WIRE
csl: apa.csl
output: rticles::arxiv_article
header-includes:
  - \usepackage{amsmath}
---


```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = 'center'
)
library(tidyverse)
```

```{r pkg}
knitr::write_bib(
  c(
    "base",
    "Lahman", 
    "pitchRx", 
    "retrosheet",
    "teamcolors",
    "ggplot2",
    "BradleyTerry2",
    "nflfastR",
    # "nflscrapr", no longer supported, can no longer be installed
    "nflverse",
    "xgboost"
  ),
  file = "pkgs.bib"
)
```

```{=latex}
\newcommand{\pkg}[1]{\textbf{#1}}
```

<!--

> Charge: Given your expertise and background, a review article from your group related to the topic of Statistical tools for sports analytics (e.g. baseball) would significantly contribute to WIREs Computational Statistics.
>
> The article should critically present the current state-of-the-art research, include opposing viewpoints, and identify challenges and opportunities.


**Remember that you are writing for an interdisciplinary audience. Please be sure to discuss interdisciplinary themes, issues, debates, etc. where appropriate.** Note that the WIREs are forums for review articles, rather than primary literature describing the results of original research.

## Article Category

- Overview


-->

# Introduction

<!--

> Introduce your topic in ~2 paragraphs, ~750 words.

> While Wiley does consider articles on preprint servers (ArXiv, bioRxiv, psyArXiv, SocArXiv, engrXiv, etc.) for submission to primary research journals, preprint articles should not be cited in WIREs manuscripts as review articles should discuss and draw conclusions only from peer-reviewed research. Remember that original research/unpublished work should also not be included as it has not yet been peer-reviewed and could put the work in jeopardy of getting published in the primary press.

-->

Insights derived from the analysis of data have transformed the world of sports over the last few decades. 
While baseball---a naturally discrete sport with more than a century's worth of professional data---may be the sport with the deepest relationship with sports analytics, one would be hard-pressed to identify a professional sport today in which sports analytics is not having an impact. 

In basketball, analytics has driven a shift in the conventional wisdom about shot selection. 
Most teams are shooting more three-pointers, settling for fewer long two-point shots, deploying more versatile defenders, and relying less on the strategy of pounding the ball into the paint in an attempt to get a high-percentage shot. [CITATION NEEDED?]
In American football, teams are going for it on fourth down far more often than in the past, a direct result of statistical analysis showing that most teams were previously overly conservative. [LOPEZ?]
And of course in baseball, teams are using defensive shifts to maximize the probability of recording an out, encouraging hitters to improve their launch angles, and optimizing pitcher repertoires to minimize contact. 

These are just the most obvious examples of strategic changes that are fueled by insights extracted from data by practitioners of sports analytics.
Similar insights are now being made in less obvious settings, including esports [@clark2020bayesian; @maymin2021smart]. 
These insights come both from academia, where researchers typically use public data to produce high-caliber, peer-reviewed scientific work, as well as from industry, where highly-trained analysts work with with players, coaches, and team officials to gain immediate effect thanks to high-resolution, often proprietary data.
A growing pool of people move seamlessly between these two worlds, leading to the formation of partnerships and the cross-pollination of ideas. 

Every sport is different, with its own set of rules, strategies, methods of data collection, number of players, and the magnitude of the role of chance. 
At the same time, many sports are similar, either because one evolved from the other, or the structure of the game shares certain attributes. 
Sports that are closely related anthropologically (NEED EXAMPLE) may or may not share common applications of analytical methods. 
Conversely, with just a few small tweaks, analytical metrics might work just as well across sports that are unrelated and quite different (e.g., an Elo rating could be equally valid for chess players and ice hockey teams). 

In this paper, we explore three key ideas that have widespread applicability across many sports: the expected value of a game state (Section \ref{sec:ev}), win probability (Section \ref{sec:wp}), and measures of team strength (Section \ref{sec:strength}). 
In each case, we define the concept mathematically, explain how it originated, and give examples of how it is applied in multiple sports. 
Our goal is to unify the conceptual threads, while doing some justice to the customizations necessary to make a metric meaningful in a particular sport. 
We include copious references to original works of scholarship.

Doing the work of sports analytics requires computing with data. 
While the sources of sports data are too numerous to list, in Section \ref{sec:tools} we highlight a few computational tools that make this kind of work possible. 

We encourage readers to explore @cochran2017oxford and @Albert2017handbook for collections of articles in sports analytics that provide broad coverage of the field. 

# The expected value of a game state {#sec:ev}

In many sports, the first step towards an analytical understanding of the game is the estimation of the expected value of a game state at any given point in a sporting contest. 

Mathematically, we define $X$ to be a random variable indicating the the number of points (or runs) that a team will score over some determined amount of time (e.g. remainder of game, quarter, period, or inning). 
Let $s \in S$ be a tuple that encodes the *state* of a game. 
Then our task is to estimate:
$$
  \mathbb{E}[X | s] = \sum_{X \geq 0} \Pr[X | s] \cdot X\,,
$$
for any state $s \in S$, where $\Pr[X | s]$ is the probability of scoring $X$ points given that the game is in state $s$ and $S$ is the set of all possible states. 

## In baseball, the expected run matrix

For clarity, we start with baseball due to the discrete nature of the sport. 
In baseball, $s$ is typically determined by two factors: the configuration of the runners on base (there are 8 possibilities) and the number of outs (3 possibilities). 
Thus, there $|S| = 24 = 8 \cdot 3$ basic states of an inning in baseball[^state], and we are often interested in the number of runs that will be scored from some state until the end of the inning. 
In this example using baseball, $\mathbb{E}[X | s] \,$ is the expected number of runs scored between now and the end of the inning given that the inning is currently in state $s$. 
The collection of estimates $\mathbb{E}[X | s]$ for all 24 states is called the *expected run matrix* [^matrix], and it is foundational in baseball analytics. 
Figure \ref{fig:lindsey} shows a reproduction of George Lindsey's original calculations and Table \ref{tab:erm} shows the expected run matrix in its more familiar form. 

[^state]: 25, if you include the absorbing state of 3 outs.

[^matrix]: There is no inherent dimensionality to $\mathbb{E}[X | s]$. The *matrix* nomenclature stems from its values typically being displayed in $8 \times 3$ grid. However, when computing with $\mathbb{E}[X | s]$, it is most often convenient to treat it as a $24 \times 1$ vector. 


```{r lindsey, out.width="90%", fig.cap="Table 1 from Lindsey's original paper. The column labelled $E(T, B)$ gives the expected run matrix as a vector, based on Lindsey's analysis of Major League Baseball data from 1959 and 1960. "}
knitr::include_graphics("lindsey_table1.png")
```


```{r erm, echo=FALSE}
lindsey <- c(
  .461, .243, .102,
  .813, .498, .219,
  1.194, .671, .297,
  1.39, .98, .355,
  1.471, .939, .403,
  1.94, 1.115, .532,
  1.96, 1.56, .687,
  2.22, 1.642, .823
)
erm <- tibble(
  base = rep(0:7, each = 3),
  out = rep(0:2, times = 8),
  runs = lindsey
)
```

```{r lindsey-erm}
int2bin <- function(x) {
  x |>
    map(intToBits) |>
    map(rev) |>
    map(tail, 3) |>
    map(str_sub, 2) |>
    map_chr(paste, collapse = "")
}
#int2bin(0:7)

erm |>
  pivot_wider(names_from = out, values_from = runs) |>
  mutate(base = int2bin(base)) |>
  rename(`base\\\\out` = base) |>
  knitr::kable(caption = "George Lindsey's expected run matrix. Note how (when reading across the rows) the expected runs decrease as outs increase for the same configuration of baserunners, while (when reading down the columns) expected runs generally increase as baserunners advance. \\label{tab:erm}")
```


Early work on this topic can be found in @lindsey1963investigation, who used play-by-play data to compute an empirical estimate for the mean number of runs scored in the remainder of the inning for each of these 24 possible states of an inning. 
This line of work led to analysis of all types of common baseball strategies. 
For example, many baseball teams elect to attempt a sacrifice bunt with a runner on first and no one out in the inning, with the goal of moving the runner to second base, at the cost of the batter being out. 
@Tango2007book (and many subsequent analyses), conclude that the sacrifice bunt is rarely worth it, because most teams would be expected to score more runs with a runner on first and no outs than they would with a runner on second and one out. 

It is worth emphasizing that the values in $\mathbb{E}[X | s]$ are estimates, and the precision of those estimates has many subtleties. 

First, the values within the expected run matrix change over time. 
For example, any estimation of the values in the expected run matrix based on data from a high-scoring era (e.g., the early 2000s) will yield different values than equivalent analysis in a low-scoring era. 
In a high run-scoring environment, where there are many home runs, the value of a walk may be higher, since a player who walks is more likely to score on a subsequent home run. 
Conversely, in a low run-scoring environment, stolen bases and sacrifice bunts may be comparatively more valuable. 
Thus, a careful estimate of $\mathbb{E}[X | s]$ would include a time parameter $t$, indicating when the estimate is appropriate. 

Second, the characterization of $S$ as having 24 states is only the simplest possible. 
The inning, or the score of the game, or even the weather, might reasonably be incorporated into $S$, as those conditions might reasonably affect the estimate of $\mathbb{E}[X | s]$. 
More definitively, the identity of the current batter, pitcher, or batter on deck, might also affect the estimate of $\mathbb{E}[X | s]$. 
Indeed, @Tango2007book show that when a particularly weak-hitting batter is up (i.e., the pitcher), a sacrifice bunt becomes a more effective strategy.  

See @Albert2001curve for a fuller discussion of the use of the expected run matrix in baseball and @marchi2018analyzing for examples of how to estimate the expected run matrix using Retrosheet data and the R statistical computing language [@R-base]. 

## In American football

The concept of estimating the value of the state of a game is easily extended to other sports. For example, in American football, $s$ is determined by factors such as down, yardage to the next first down, time remaining in the game, and field position. 

The first effort of estimating expected points of possession in football goes back to @Carter1971operations. In this technical note, the authors estimate the expected points for 1st and 10 plays in the NFL given any yard line on the football field. Due to limitations regarding the amount of data collected, the football field is broken into 10-yard strips, centered at their midpoints (e.g. 5, 15, 25, 35, etc.), and the expected points are calculated for each of those field positions. Other early work on expected point values in American football can be found in @Carroll1988hidden, which popularized the idea of play evaluation using expected points and expected points added.

@goldner2012markov and @goldner2017situational propose a Markov model for estimating expected points in football. 
In particular, the author consider a football drive as an *absorbing Markov chain*, consisting of distinct *absorbing states* touchdowns, field goals, and other play outcomes. 
For any given play, the expected points is calculated using the absorption probabilities for different scoring events.

A more in-depth overview of the history of expected points is provided in @Yurko2019nflwar (Section 1.1). 
Most importantly, this paper uses publicly available data provided by the **nflscrapR** package [@R-nflscrapR] to model the expected points on a play-by-play level in football.

The authors introduce a multinomial logistic regression approach, which takes into account the current down, time remaining, yards from endzone, yards to go, and the indicators for goal to go and whether the time remaining in the half is less than two minutes. 

This model estimates the probabilities of the following possible scoring outcomes after each play: no score, safety, field goal, and touchdown for both the offensive and defensive teams, all of which have a point value. The expected points for a play can then be calculated accordingly, by taking the sum of the products of the scoring event point values and their associated probabilities.

### 4th down strategy

(A paragraph on applications of expected points on decision making, fourth down strategy, with an example)

4th down strategy papers based on expected points.  
@Yam2019: 
@Romer2006: Examines 4th down decisions in the NFL using expected points.  Focuses only on the first quarter.  Teams don't go for it enough is they are trying to maximize their probability of winning the game.  
@pelechrinis2019 Present an expected points framework where they adjust for the strength of the defense.  
<!-- New york times article: https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html?_r=0 -->

@Alamar2010 Evaluation of play calling in the NFL based on expected points.  
https://www.degruyter.com/document/doi/10.2202/1559-0410.1235/html

@urschel2011 Examines decsions on kick offs (surprise on sides or regular kick off AND accept touchback or return kick) using an expected points frame work.  Shows that teams are moderately risk and loss averse.

@White2002 Evaluated quaraterback usein gtiered multinomial regression in an expected points framework. They use a simple expected points model with down, to go, and to goal as predictors.   
https://www.eng.buffalo.edu/~jzhuang/Papers/JQAS_NFL_2011.pdf


Numerous papers (see @lopez2020bigger for details) have used the analysis of the expected number of points to improve fourth down strategy. 


## In sports with continuous states

Even in sports where the concept of a state is not so easy to define, the value of a possession can be estimated with the help of tracking data. 
Over the past decade or so, professional sports leagues have collected tracking data which record the locations of all players and the ball (or puck) throughout a game. 
This high-resolution data allows researchers to produce advanced analyses of the captured spatio-temporal information and better understand the game. 
This is a great leap forward from older resources such as traditional box-score results and play-by-play data.

In basketball, @cervone2014pointwise and @cervone2016multiresolution introduce expected possession value (EPV) as means toward an assessment of a player's on-court performance. 
This metric is a continuous-time estimate of the expected number of points for the offensive team on a given possession using player and ball locations. 
The EPV takes into account all possible outcomes (a shot attempt, a pass, etc.) for a given player with the ball, with different weights being assigned to each decision. 
Consequently, the authors derive a metric called EPV-Added (EPVA), measuring a player's EPV contribution in a given situation relative to a league-average player.

(include code demo? https://github.com/dcervone/EPVDemo)
> It might be cool to include one of their graphics here, if can. Or regenerate one. 

Another framework for estimating expected points in basketball is proposed by @Sicilia2019DeepHoops. 
The authors offer a different point of view on expected points, where they first consider a classification model which returns the probabilities for whether a player would commit a foul (shooting and non-shooting), turnover, or attempt a shot. 
The values associated with each of those "terminal actions" are then used to compute the expected points within a basketball play.

See also @bornn2017studying for more information on how tracking data have enabled advanced statistical analyses of basketball in recent years.

In American football, @Yurko2020going use tracking data provided by the [2019 NFL Big Data Bowl](https://operations.nfl.com/gameday/analytics/big-data-bowl/past-big-data-bowl-recaps/) to model the expected yards gained for a ball-carrier during the course of a play. 
As an extension to pre-existing approaches, the authors use conditional density estimation to obtain a probability distribution for the number of yards gained during the play, rather than only producing a single estimate for the expected yards gained. 
Accordingly, the probability of various types of outcomes at the end of a play such as a touchdown or a first-down gain can be computed from the yards gained distribution.

The notion of EPV has also been extended to soccer.
@fernandez2021framework implement deep learning methods to examine the instantaneous expected value of soccer possessions. 
This approach considers passes, ball drives, and shots in soccer as the main set of actions used to compute expected possession value. 


## Optimal strategies that don't maximize expected points

In Section \ref{sec:ev}, we defined the expected value of a possession based on the state $s$ of the game in terms of the expected number of points (runs) $X$ that would be scored in the remainder of some period of time. 
We then showed how this value could be used to analyze the relative effectiveness of certain strategies, with the simple idea that strategies that yield higher expected values are preferable. 
Generally, the goal of any sport is to score more points than the other team, which most often means trying to score as many points as possible, leading to a general strategy of maximizing expected points. 
However, there are situations in which maximizing the number of expected points is *not* the desired strategy. 

For example, in the bottom of the ninth inning of a tied baseball game, the optimal strategy for winning the game is maximizing the probability of scoring *at least one run*, which may differ from the strategy of maximizing expected runs. 
If we let $U$ be the set of all strategies, then we assert that it is not always the case that the strategy $u$ that maximizes the expected number of points will maximize the probability of winning:
$$
  \underset{u \in U}{\arg \max \,}{\Pr[X > 0 | s, u]} \neq \underset{u \in U}{\arg \max \,}{\mathbb{E}[X | s, u]} \,.
$$
Consider the situation where runners are on first and third base, and the score is tied in the bottom of the ninth inning with no one out. 
Table \ref{tab:tied} reveals that the expected number of runs scored in the remainder of the inning is 1.94 runs, while the the probability of scoring zero runs is 0.13. 
The defense is in a tight spot, facing an 87% probability of losing the game. 
However, by walking the hitter to load the bases, they create the opportunity to force the lead runner at home and thus reduce the chances of scoring to 82%, even though they raise the expected number of runs scored to 2.22. 
In this case, the defensive team is wise to pursue the strategy of maximizing the expected number of runs scored, because it *minimizes* the probability of scoring at least one run. 

```{r}
erm <- erm |>
  mutate(
    pr_0 = c(
      .747, .855, .933,
      .604, .734, .886, 
      .381, .610, .788,
      .120, .307, .738,
      .395, .571, .791,
      .130, .367, .717,
      .180, .270, .668,
      .180, .303, .671
    )
  )

erm |>
  filter(out == 0, base %in% c(5, 7)) |>
  mutate(base = int2bin(base)) |>
  knitr::kable(caption = "Loading the bases can be a rational strategy. In this case, loading the bases maximizes both the expected number of runs scored *and* the probability of not scoring at all. \\label{tab:tied}")
```

Maximizing the probability of scoring is optimal in any sudden-death situation, which has (but currently does not) included overtime in American football [@martin2018markov]. 

The situation gets even more interesting when teams modify both their offensive and defensive strategies simultaneously. 
For example, in hockey teams will often pull their goalie when trailing in the final period. 
This strategy severely weakens their defense, but strengthens their offense. 
The hope is to score a quick goal to get back in the game, but the risk is falling further behind. 
@beaudoin2010strategies show that NHL coaches do not always employ the optimal strategies. 
@skinner2011scoring develops a general framework for these desperation strategies, which include the onside kick in American football, pulling the infield and/or outfield in in baseball, and of course, the fabled Hack-a-Shaq strategy in basketball. 


# Win probability {#sec:wp}

A related, but different concept is the notion of *win probability*.
Win probability is simply an estimate of the probability that a team will win the game, given its current state $s$. 
Extending the mathematical framework we defined in Section \ref{sec:ev}, let $W_i$ be a binary random variable that indicates a win for team $i$. 
Then, 
$$
  \Pr[W_i | s] \,,
$$
is the win probability for team $i$ in the state $s$. 

This win probability is closely related to the expected value of a state. 
@Albert2015 defines the win probability as:
$$
  \Pr[W_i | s] = \sum_{X \geq 0} \Pr[X | s] \cdot \Pr[W_i | X, s] \,,
$$
where $\Pr[W_i | X, s]$ is the probability that team $i$ will win the game given that they score $X$ points from state $s$. 

Win probability is easily extended to provide a measure of the impact of sports plays and individual player contributions, as discussed in @Albert2015. 
Given its popularity, recent books on sports analytics often dedicate multiple chapters entirely to win probability. 
These include @Albert2001curve, @Schwarz2005numbers, @Tango2007book, @Albert2017handbook, and @Winston2022. 

In this section, we discuss notable previous work on win probability in baseball, American football, basketball, and several other sports. 

## Baseball

@Albert2015 provides a historical overview of the use of win probability in baseball. 
This notion goes back to at least as early as @Lindsey1961, where the expected win probability after each inning was calculated based on the distribution of runs scored in each inning. 
@Mills1970 utilize win probability to introduce Player Win Average (PWA), a measure of a player's contribution to the game outcome. 
In particular, PWA is computed as 
$$
PWA = \frac{Win \ Points}{Win \ Points + Loss \ Points},
$$
where the win and loss points represent how much the player positively and negatively impact their team's probability of winning after each play.
In effect, the win points are the sum of the changes in $\Pr[W_i | s]$ from one state to the next. 

## American football

In recent years, a number of statistical methods have been used to build well-calibrated win probability models in American football. 
These are flexible models that have high predictability, can account for nonlinear interactions between the explanatory variables, require few assumptions, and produce feature importance scores.

@Lock2014 use random forests as a means for estimating win probability before each play in a football game. 
Covariates included in this win probability model are the current down, score differential, time remaining, adjusted score, point spread, number of timeouts remaining for each team, total points scored, current yardline, and yards to go for a first down. 

In addition, @Yurko2019nflwar estimate win probability in the NFL using a generalized additive model (GAM), as part of the **nflscrapR** package [@R-nflscrapR] and nflWAR framework. 
This model takes into account the estimated expected points obtained from the model described in Section \ref{sec:ev}, along with other predictors for time, current half, and timeouts. 
The two win probability frameworks proposed by @Lock2014 and @Yurko2019nflwar were also implemented in @Yam2019 with minimal modifications. Specifically, the authors combined both approaches to estimate the win probability for each play, with an overall goal of assessing fourth down decision-making in gridiron football.

A vital highlight of @Yurko2019nflwar's win probability model is that it is fully reproducible and uses publicly available data. 
One of @Yurko2019nflwar's goals was also to encourage researchers to "use, explore, and improve upon our work," which ultimately inspired \pkg{nflfastR} (@R-nflfastR), now considered the successor to **nflscrapR**. 

Figure \ref{fig:nflfastr_wp} shows a win probability graph for the January 14, 2018 NFL Playoffs Divisional Round matchup between the New Orleans Saints and the Minnesota Vikings. 
We obtain the estimated probability of winning for each team using the \pkg{nflfastR} R package, which implements a gradient boosting model via the \pkg{xgboost} library (@R-xgboost) for estimating win probabilities. 
Minnesota was leading throughout the first three quarters of the game, having win probabilities of 0.869, 0.941, 0.742 at the end of the first, second, and third quarters, respectively. 
The win probabilities get close to parity late in the fourth quarter, when the Saints took the lead with 3:01 left in the game. 
The last play of this game---famously known as the [Minneapolis Miracle](https://en.wikipedia.org/wiki/Minneapolis_Miracle)---resulted in a drastic swing in win probabilities for both teams. 
With 10 seconds remaining in the game, the Vikings begin the final possession with a 25.3% chance of winning. 
Their probability increased to a perfect 1 when Stefon Diggs scored a game-winning 61-yard receiving touchdown as the game clock expired.

```{r}
# win prob for Minneapolis Miracle
library(teamcolors)

miracle <- teamcolors |> 
  filter(league == "nfl", grepl("Vikings|Saints", name)) |>
  arrange()
```

```{r nflfastr_wp, fig.cap="Win probability graph for New Orleans Saints vs. Minnesota Vikings in the 2017--18 NFL Playoffs."}
library(nflfastR)
x <- load_pbp(2017) |> 
  filter(
    season_type == "POST",
    home_team == "MIN",
    away_team == "NO",
    !is.na(home_wp),
    !is.na(away_wp)
  ) |>
  select(game_seconds_remaining, home_wp, away_wp) |> 
  pivot_longer(
    !game_seconds_remaining,
    names_to = "team",
    values_to = "wp"
  )

wp_plot <- ggplot(x, aes(game_seconds_remaining, wp, color = team)) +
  scale_color_manual(
    values = setNames(miracle$primary, c("home_wp", "away_wp")), 
    labels = miracle$name
  ) +
  geom_hline(yintercept = 0.5, color = "gray", linetype = "dashed") +
  geom_vline(
    xintercept = seq(0, 3600, 900), 
    linetype = "dashed", 
    color = "gray"
  ) + 
  geom_line(size = 1.5, alpha = 0.8) +
  labs(
    x = "Time Remaining (seconds)",
    y = "Win Probability",
    color = NULL
  ) +
  theme_bw() +
  theme(legend.position = "top")

library(magick)
logos <- miracle |>
  pull(logo) |>
  map(image_read) |>
  map(as.raster)

wp_plot + 
  annotation_raster(logos[[1]], -1010, -1490, 0.76, 0.865) + 
  annotation_raster(logos[[2]], -1010, -1490, 0.135, 0.24) + 
  scale_x_reverse()
```


## Basketball

@Stern1994 uses Brownian motion to investigate the scoring process of basketball. 
Let $p(l, t)$ represent the win probability for the home team given an $l$-point lead after $t$ seconds of game time. 
The model introduced by @Stern1994 is a probit regression model, which provides an estimate for $p(l, t)$. 
In particular, 
$$
p(l, t) = \Phi\left(\frac{l + (1-t)\mu}{\sqrt{(1-t)\sigma^2}}\right)
\,.
$$
Here, a Brownian motion process with drift $\mu$ points advantage for the home team and variance $\sigma^2$ is used to model the score difference between the home and away teams. 

On a related note, @Deshpande2016 extend @Stern1994's framework by applying it in a Bayesian setting. 
In particular, they propose a Bayesian lasso regression model to assess the impact of individual players on their team's win probability at a given time of a basketball game. 
This model assumes independence of observation, constant variability in win probability, and also addresses multicollinearity.

@McFarlane2019 uses logistic regression to estimate win probability for evaluating end-of-game decisions in the NBA. 
The approach takes into account the remaining game time, score difference, and point spread. 
This basketball win probability model is then applied to the calculation of the End-of-game Tactics Metric (ETM), measuring how the win probability differs between the optimal and on-court actual decisions.

## Other sports

The idea of win probability is also applied to other sports, with a diverse range of statistical techniques being used to estimate the probability of winning for a player or team. 
@Brenzel2019 uses three-dimensional Markov models to estimate win probability throughout a curling match. 
In particular, the authors propose both homogeneous and heterogeneous Markov models for estimating the chance of winning in curling, with different independence assumptions on the relationship between performance and the current state of the game. 
In esports, @maymin2021smart uses logistic regression to build a well-calibrated in-game win probability model for each specific moment during a game of League of Legends. 
@Guan2022 develops an in-game win probability model for the National Rugby League using functional data analysis. 
In this approach, the rugby event data are treated as functional, and the win probability is expressed as a function of the match time. 

# Team strength {#sec:strength}

A third crucial idea in sports analytics is the estimation of team strength. 
A popular method for estimating team strength in sports is through pairwise evaluations. 
Perhaps the most famous probability model for predicting the outcome of a paired comparison is the Bradley-Terry model (BTM) [@Bradley1952]. 
For a pair of players (or teams) $i$ and $j$, let $\Pi_{ij}$ denote the probability that $i$ is preferred to $j$. 
Then the BTM is a logistic regression model with parameters $\beta_i, \beta_j$ such that

$$
\log\left(\frac{\Pi_{ij}}{\Pi_{ji}}\right) = \beta_i - \beta_j \,.
$$

Here, $\exp{\beta_i}$ is often viewed as a representation of player $i$'s ability. 

The BTM can be implemented in `R` [@R-base] via the \pkg{BradleyTerry2} package [@R-BradleyTerry2]. 
As an example, we consider the data given in @agresti2018introduction (page 247) on tennis results from 2014--2018 for five men's professional players: Novak Djokovic, Roger Federer, Andy Murray, Rafael Nadal, and Stan Wawrinka. 
We fit a BTM to estimate the win probability for each pair of players and obtain a ranking for this group of five.

Table \ref{tab:tennisbtm} shows the estimated coefficients of the fitted BTM. 
According to the abilities, between 2014 and 2018 the players are ranked as follows: 1) Djokovic, 2) Federer, 3) Wawrinka, 4) Nadal, 5) Murray.
To obtain win probabilities, as an illustration, for the Federer-Nadal matchup, an estimate for the probability of a Federer victory is:
$$
\hat\Pi_{24} = \frac{exp(\hat\beta_2 - \hat\beta_4)}{1 + exp(\hat\beta_2 - \hat\beta_4)} = \frac{exp(1.136 + 0.062)}{1 + exp(1.136 + 0.062)} = 0.768
$$

```{r}
tennis <- dplyr::tibble(
  Djokovic = c(NA, 9, 14, 9, 4),
  Federer = c(6, NA, 5, 5, 7),
  Murray = c(3, 0, NA, 2, 2),
  Nadal = c(2, 1, 4, NA, 4),
  Wawrinka = c(3, 2, 2, 3, NA)
) |>
  t() |>
  BradleyTerry2::countsToBinomial()

tennis_bt <- BradleyTerry2::BTm(cbind(win1, win2), player1, player2, data = tennis)

tennis_bt |>
  update(refcat = "Wawrinka") |> 
  BradleyTerry2::BTabilities() |> 
  round(3) |> 
  knitr::kable(caption = "Estimated abilities for each player, relative to Wawrinka, obtained from the fitted Bradley-Terry model. \\label{tab:tennisbtm}")
```

```{r btm, eval=FALSE}
library(BradleyTerry2)
# show BTM for Pirates vs. Yankees in 1960
# or Red Sox vs. Yankees in 2003/2004
library(retrosheet)
games <- get_retrosheet("game", 2004)

bt_games <- games |>
  group_by(HmTm, VisTm) |>
  summarize(
    home_w = sum(HmRuns > VisRuns),
    away_w = sum(VisRuns > HmRuns),
  ) |>
  mutate(
    HmTm = factor(HmTm),
    VisTm = factor(VisTm, levels = levels(HmTm))
  )

btm <- BTm(
  outcome = cbind(home_w, away_w), 
  player1 = HmTm, 
  player2 = VisTm,
  data = bt_games, 
  id = "team"
)
btm

bt_games2 <- bt_games |>
  mutate(
    HmTm = data.frame(team = HmTm, at_home = 1),
    VisTm = data.frame(team = VisTm, at_home = 0)
  )

btm2 <- btm |>
  update(formula = ~ team + at_home, data = bt_games2)
btm2

btm |>
  broom::tidy() |>
  arrange(desc(estimate))
btm2 |>
  broom::tidy() |>
  arrange(desc(estimate))
```

Another widely known tool for measuring team strength is the Elo rating system [@Elo1978]. The Elo rating was originally developed for chess, but also has been used to estimate team strength in other sports. 
See @koning2017rating for more information on applications of the Elo rating in soccer. 

@Glickman1998 propose a Bayesian state-space model for paired comparisons for predicting NFL games, allowing team strength parameters to vary over time. 
In particular, they model point differential in the NFL by introducing week-to-week and season-to-season as the two primary sources variation in team strengths. 
See also @glickman2017estimating for more discussion on estimating team strengths in American football. 
More recently, @Lopez2018 extend @Glickman1998's state-space model to understand randomness in the four major American sports leagues. 
Betting moneylines are used in place of point differentials in order to estimate team strengths, and this framework also accounts for home advantage.

# Tools {#sec:tools}


See @baumer2022ctv for R packages useful for sports analytics. 

@marchi2018analyzing

@R-Lahman, @R-pitchRx, @R-retrosheet

Figure \ref{fig:chess-startup} shows a rendering of the starting chess board obtained via the \pkg{chess} package. 

```{r chess-startup, fig.cap="The starting chess board printed via the \\pkg{chess} package."}
#library(chess)
#Sys.setenv(RETICULATE_PYTHON = "my_env/bin/python3")
#game() %>% plot()
```

@R-teamcolors provides color palettes for professional and amateur sports teams, as well as color and fill scale functions compatible with \pkg{ggplot2} [@R-ggplot2].
The team colors and logos shown in Figure \ref{fig:nflfastr_wp} were provided by the \pkg{teamcolors} package. 
Figure \ref{fig:teamcolors} illustrates team color palettes for professional sports leagues. 

```{r teamcolors, warning=FALSE, fig.cap="Professional sports team color palettes provided by the \\pkg{teamcolors} package. ", fig.height=9}
teamcolors::show_team_col()
```


# Conclusion

Sum up the key conclusions of your review, highlighting the most promising scientific developments, directions for future research, applications, etc. The conclusion should be ~2 paragraphs, ~750 words total.



# Acknowledgments

We are grateful to Michael Lopez and Katherine Evans for their thoughts on early versions of this paper. 

# References {-}
