---
title: "Big Ideas in Sports Analytics and Statistical Tools for their Investigation"
authors:
  - name: Benjamin S. Baumer
#    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    department: Statistical & Data Sciences
    affiliation: Smith College
    location: Northampton, MA 01063
    email: bbaumer@smith.edu
    ORCID: 0000-0002-3279-0516
  - name: Gregory J. Matthews
    department: Mathematics and Statistics
    affiliation: Loyola University Chicago
    location: Chicago, IL 60660
    email: gmatthews1@luc.edu
    ORCID: 0000-0002-8413-5097
  - name: Quang Nguyen
    department: Statistics & Data Science
    affiliation: Carnegie Mellon University
    location: Pittsburgh, PA 15213
    email: nmquang@cmu.edu
abstract: |
  Sports analytics---broadly defined as the pursuit of improvement in athletic performance through the analysis of data---has expanded its footprint both in the professional sports industry and in academia over the past 30 years. We connect three big ideas that are common across multiple sports, and explore both the shared similarities and individual idiosyncracies of analytical approaches in each sport. While our focus is on the concepts underlying each type of analysis, any implementation necessarily involves statistical methodologies, computational tools, and sources of data. Where appropriate, we outline how data, models, tools, and knowledge of the sport combine to generate actionable insights. This paper should serve as a useful overview for anyone becoming interested in the study of sports analytics. 
keywords:
  - sports analytics
  - R packages
  - sports data
  - pairwise comparisons
  - datasets
bibliography: [refs.bib, pkgs.bib]
biblio-style: unsrt   
# use apa for WIRE
csl: apa.csl
output: rticles::arxiv_article
header-includes:
  - \usepackage{amsmath}
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = 'center'
)
library(tidyverse)
library(kableExtra)
```

```{r pkg}
knitr::write_bib(
  c(
    "base",
    "Lahman", 
    "pitchRx", 
    "retrosheet",
    "teamcolors",
    "ggplot2",
    "BradleyTerry2",
    "nflfastR",
    # "nflscrapr", no longer supported, can no longer be installed
    "nflverse",
    "xgboost",
    "chess", 
    "chessR"
  ),
  file = "pkgs.bib"
)

xfun::gsub_file(
  "pkgs.bib",
  "\\{Jason Zivkovic\\}",
  "Zivkovic, Jason"
  )
```

```{=latex}
\newcommand{\pkg}[1]{\textbf{#1}}
```

<!--

> Charge: Given your expertise and background, a review article from your group related to the topic of Statistical tools for sports analytics (e.g. baseball) would significantly contribute to WIREs Computational Statistics.
>
> The article should critically present the current state-of-the-art research, include opposing viewpoints, and identify challenges and opportunities.


**Remember that you are writing for an interdisciplinary audience. Please be sure to discuss interdisciplinary themes, issues, debates, etc. where appropriate.** Note that the WIREs are forums for review articles, rather than primary literature describing the results of original research.

## Article Category

- Overview


-->

# Introduction

<!--

> Introduce your topic in ~2 paragraphs, ~750 words.

> While Wiley does consider articles on preprint servers (ArXiv, bioRxiv, psyArXiv, SocArXiv, engrXiv, etc.) for submission to primary research journals, preprint articles should not be cited in WIREs manuscripts as review articles should discuss and draw conclusions only from peer-reviewed research. Remember that original research/unpublished work should also not be included as it has not yet been peer-reviewed and could put the work in jeopardy of getting published in the primary press.

-->

Insights derived from the analysis of data have transformed the world of sports over the last few decades. 
While baseball---a naturally discrete sport with more than a century's worth of professional data---may be the sport with the deepest relationship with sports analytics, one would be hard-pressed to identify a professional sport today in which sports analytics is not having an impact. 

In basketball, analytics has driven a shift in the conventional wisdom about shot selection. 
Most teams are shooting more three-pointers, settling for fewer long two-point shots, deploying more versatile defenders, and relying less on the strategy of pounding the ball into the paint in an attempt to get a high-percentage shot [@Schuhmann2021nba3point].
In American football, teams are going for it on fourth down far more often than in the past, a direct result of statistical analysis showing that most teams were previously overly conservative [@lopez2020bigger].
And of course in baseball, teams are using defensive shifts to maximize the probability of recording an out, encouraging hitters to improve their launch angles, and optimizing pitcher repertoires to minimize contact. 

These are just the most obvious examples of strategic changes that are fueled by insights extracted from data by practitioners of sports analytics.
Similar insights are now being made in less obvious settings, including esports [@clark2020bayesian; @maymin2021smart]. 
These insights come both from academia, where researchers typically use public data to produce high-caliber, peer-reviewed scientific work, as well as from industry, where highly-trained analysts work with with players, coaches, and team officials to gain immediate effect thanks to high-resolution, often proprietary data.
A growing pool of people move seamlessly between these two worlds, leading to the formation of partnerships and the cross-pollination of ideas. 

Every sport is different, with its own set of rules, strategies, methods of data collection, number of players, and the magnitude of the role of chance. 
At the same time, many sports are similar, either because one evolved from the other, or the structure of the game shares certain attributes. 
Sports that are closely related anthropologically may or may not share common applications of analytical methods. 
For example, despite belonging to the same bat-and-ball family, baseball and cricket differ in strategies such as batting order or sacrifice plays.
Conversely, with just a few small tweaks, analytical metrics might work just as well across sports that are unrelated and quite different.
For instance, an Elo rating could be equally valid for chess players and ice hockey teams. 

In this paper, we explore three key ideas that have widespread applicability across many sports: the expected value of a game state (Section \ref{sec:ev}), win probability (Section \ref{sec:wp}), and measures of team strength (Section \ref{sec:strength}). 
In each case, we define the concept mathematically, explain how it originated, and give examples of how it is applied in multiple sports. 
Our goal is to unify the conceptual threads, while doing some justice to the customizations necessary to make a metric meaningful in a particular sport. 
We include copious references to original works of scholarship.

Doing the work of sports analytics requires computing with data. 
While the sources of sports data are too numerous to list, in Section \ref{sec:tools} we highlight a few computational tools that make this kind of work possible. 

We encourage readers to explore @cochran2017oxford and @Albert2017handbook for collections of articles in sports analytics that provide broad coverage of the field. 

# The expected value of a game state {#sec:ev}

In many sports, the first step towards an analytical understanding of the game is the estimation of the expected value of a game state at any given point in a sporting contest. 
Mathematically, we define $X$ to be a random variable indicating the the number of points (or runs) that a team will score over some determined amount of time (e.g. remainder of game, quarter, period, or inning). 
Let $s \in S$ be a tuple that encodes the *state* of a game. 
Then our task is to estimate:
$$
  \mathbb{E}[X | s] = \sum_{X \geq 0} \Pr[X | s] \cdot X\,,
$$
for any state $s \in S$, where $\Pr[X | s]$ is the probability of scoring $X$ points given that the game is in state $s$ and $S$ is the set of all possible states. 

## In baseball, the expected run matrix

For clarity, we start with baseball due to the discrete nature of the sport. 
In baseball, $s$ is typically determined by two factors: the configuration of the runners on base (there are 8 possibilities) and the number of outs (3 possibilities). 
Thus, there $|S| = 24 = 8 \cdot 3$ basic states of an inning in baseball[^state], and we are often interested in the number of runs that will be scored from some state until the end of the inning. 
In this example using baseball, $\mathbb{E}[X | s] \,$ is the expected number of runs scored between now and the end of the inning given that the inning is currently in state $s$. 
The collection of estimates $\mathbb{E}[X | s]$ for all 24 states is called the *expected run matrix* [^matrix], and it is foundational in baseball analytics. 
Figure \ref{fig:lindsey} shows a reproduction of George Lindsey's original calculations and Table \ref{tab:erm} shows the expected run matrix in its more familiar form. 

[^state]: 25, if you include the absorbing state of 3 outs.

[^matrix]: There is no inherent dimensionality to $\mathbb{E}[X | s]$. The *matrix* nomenclature stems from its values typically being displayed in $8 \times 3$ grid. However, when computing with $\mathbb{E}[X | s]$, it is most often convenient to treat it as a $24 \times 1$ vector. 


```{r lindsey, out.width="90%", fig.cap="Table 1 from Lindsey's original paper. The column labelled $E(T, B)$ gives the expected run matrix as a vector, based on Lindsey's analysis of Major League Baseball data from 1959 and 1960. "}
knitr::include_graphics("lindsey_table1.png")
```


```{r erm, echo=FALSE}
lindsey <- c(
  .461, .243, .102,
  .813, .498, .219,
  1.194, .671, .297,
  1.39, .98, .355,
  1.471, .939, .403,
  1.94, 1.115, .532,
  1.96, 1.56, .687,
  2.22, 1.642, .823
)
erm <- tibble(
  Base = rep(0:7, each = 3),
  Out = rep(0:2, times = 8),
  Runs = lindsey
)
```

```{r lindsey-erm}
int2bin <- function(x) {
  x |>
    map(intToBits) |>
    map(rev) |>
    map(tail, 3) |>
    map(str_sub, 2) |>
    map_chr(paste, collapse = "")
}
#int2bin(0:7)

erm |>
  pivot_wider(names_from = Out, values_from = Runs) |>
  mutate(Base = int2bin(Base)) |>
  kable(booktabs = TRUE, 
        linesep = "",
        caption = "George Lindsey's expected run matrix. Note how (when reading across the rows) the expected runs decrease as outs increase for the same configuration of baserunners, while (when reading down the columns) expected runs generally increase as baserunners advance. \\label{tab:erm}") |> 
  add_header_above(c("", "Out" = 3))
```


Early work on this topic can be found in @lindsey1963investigation, who used play-by-play data to compute an empirical estimate for the mean number of runs scored in the remainder of the inning for each of these 24 possible states of an inning. 
This line of work led to analysis of all types of common baseball strategies. 
For example, many baseball teams elect to attempt a sacrifice bunt with a runner on first and no one out in the inning, with the goal of moving the runner to second base, at the cost of the batter being out. 
@Tango2007book (and many subsequent analyses), conclude that the sacrifice bunt is rarely worth it, because most teams would be expected to score more runs with a runner on first and no outs than they would with a runner on second and one out. 

It is worth emphasizing that the values in $\mathbb{E}[X | s]$ are estimates, and the precision of those estimates has many subtleties. 

First, the values within the expected run matrix change over time. 
For example, any estimation of the values in the expected run matrix based on data from a high-scoring era (e.g., the early 2000s) will yield different values than equivalent analysis in a low-scoring era. 
In a high run-scoring environment, where there are many home runs, the value of a walk may be higher, since a player who walks is more likely to score on a subsequent home run. 
Conversely, in a low run-scoring environment, stolen bases and sacrifice bunts may be comparatively more valuable. 
Thus, a careful estimate of $\mathbb{E}[X | s]$ would include a time parameter $t$, indicating when the estimate is appropriate. 

Second, the characterization of $S$ as having 24 states is only the simplest possible. 
The inning, or the score of the game, or even the weather, might reasonably be incorporated into $S$, as those conditions might reasonably affect the estimate of $\mathbb{E}[X | s]$. 
More definitively, the identity of the current batter, pitcher, or batter on deck, might also affect the estimate of $\mathbb{E}[X | s]$. 
Indeed, @Tango2007book show that when a particularly weak-hitting batter is up (i.e., the pitcher), a sacrifice bunt becomes a more effective strategy.  

See @Albert2001curve for a fuller discussion of the use of the expected run matrix in baseball and @marchi2018analyzing for examples of how to estimate the expected run matrix using Retrosheet data and the R statistical computing language [@R-base]. 

## In American football

The concept of estimating the value of the state of a game is easily extended to other sports. 
For example, in American football, $s$ is determined by situational variables at the start of a play such as down, yardage to the next first down, time remaining in the game, and field position. 

The first effort of estimating expected points of possession in football goes back to @Carter1971operations. 
In this technical note, the authors estimate the expected points for 1st and 10 plays in the NFL given any yard line on the football field. 
Due to limitations regarding the amount of data collected, the football field is broken into 10-yard buckets, centered at their midpoints (e.g. 5, 15, 25, 35, etc.), before averaging the value of the next scoring instance across the field to obtain the expected points.

Other early work on expected point values in American football can be found in @Carroll1988hidden. 
In particular, the authors extend @Carter1971operations's work and propose a linear model for expected points in the NFL.
It is determined that every extra 25 yards is associated with 2 more points scored on average for a football team.

@goldner2012markov and @goldner2017situational propose a Markov model for estimating expected points in football. 
In particular, the author considers a football drive as an *absorbing Markov chain*, consisting of distinct *absorbing states* touchdowns, field goals, and other play outcomes. 
For any given play, the expected points is calculated using the absorption probabilities for different scoring events.

A more in-depth overview of the history of expected points in sports is provided in @Yurko2019nflwar (Section 1.1). 
Most importantly, this paper uses publicly available data provided by the **nflscrapR** package [@R-nflscrapR] to model the expected points on a play-by-play level in football.

The authors introduce a multinomial logistic regression approach, which takes into account the current down, time remaining, yards from endzone, yards to go, and the indicators for goal to go and whether the time remaining in the half is less than two minutes. 
This model estimates the probabilities of the following possible scoring outcomes after each play: no score, safety, field goal, and touchdown for both the offensive and defensive teams, all of which have a point value.
The expected points for a play can then be calculated accordingly, by summing up the products of the scoring event point values and their associated probabilities.

@pelechrinis2019 develops an expected points framework in the same spirit as previous work, but accounts for the strength of the opponents in their method.  They state that by failing to account for opponent strength appropriately, about 124.8 points per team each season (or about 3.8 wins per season) are not credited correctly.  

### 4th down strategy

The concept of expected points in American football has many applications.  One of the most notable and well-studied is the use of expected points in the evaluation of 4th down strategy, and there is near universal consensus in the literature that NFL teams have been too conservative when making 4th down decisions.  
@Romer2006 examines 4th down decisions in the NFL using expected points by focusing only on examples from the first quarter of a game (to avoid issues with end-of-half and end-of-game decision making). 
They conclude that teams don't go for it enough if they are trying to maximize their probability of winning the game. 

Numerous papers (see @lopez2020bigger for details) use the analysis of the expected number of points to improve fourth down strategy. 
In addition to @Romer2006, later work by @Yam2019 uses win probability (see Section \ref{sec:wp}), rather than expected points, and a casual inference framework to reach similar conclusions that NFL teams are to conservative in going for it on 4th down. 
In addition, they estimate that a better strategy would be worth about 0.4 wins per season on average, a substantial amount in a season with only 16 games. 

Recently, @lopez2020bigger presented an introduction to NFL tracking data, and as an example they examine 4th down behavior.  Rather than looking at the traditional distance to go, which is rounded, they have the exact position of the ball leading to an exact distance to go.  They demonstrate than teams are still conservative when choosing to go for it or punt on 4th down, but the effect may be overstated. 

### Other applications of expected points in American football

@White2002 present a tiered logistic regression method that can be, in general, applied to any regression setting with a polychotomous response.  
Using this technique, they estimate the value of NFL plays using a simple expected points model with down, yards to go, and yards to goal as predictors.
They then use these results to evaluate and rank NFL quarterbacks.  

@Alamar2010 uses an expected points framework to evaluate play calling in the NFL. 
However, rather than evaluate each play on its own, they evaluate the play in the context of the drive. 
Based on play-by-play data from 2005 through 2008, they find that teams are underutilizing passing plays in some situations.  

@urschel2011 use expected points to evaluate NFL coaching decisions on kickoffs. 
Specifically, they look at surprise on-sides kicks vs. regular kickoff and the decision to accept a touchback vs. returning the kickoff. 
Using data from the 2009 NFL season, the conclude, as many have, that coaches in the NFL tend to make conservative decisions.  



<!-- decsions on kick offs (surprise on sides or regular kick off AND accept touchback or return kick) using an expected points frame work.  Shows that teams are moderately risk and loss averse. -->
<!-- https://www.eng.buffalo.edu/~jzhuang/Papers/JQAS_NFL_2011.pdf -->

<!-- (A paragraph on applications of expected points on decision making, fourth down strategy, with an example) -->


<!-- 4th down strategy papers based on expected points.   -->
<!-- @lopez2020bigger: Talks about how tracking data is important for a more nuanced discussion of 4th down decisions.   -->
<!-- @Yam2019: Uses Causal inference techniques to examine the effects of 4th down decision making in the NFL.   -->
<!-- @Romer2006: Examines 4th down decisions in the NFL using expected points.  Focuses only on the first quarter.  Teams don't go for it enough is they are trying to maximize their probability of winning the game.   -->
<!-- @pelechrinis2019 Present an expected points framework where they adjust for the strength of the defense.   -->
<!-- New york times article: https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html?_r=0 -->

<!-- @Alamar2010 Evaluation of play calling in the NFL based on expected points.   -->
<!-- https://www.degruyter.com/document/doi/10.2202/1559-0410.1235/html -->






<!-- @urschel2011 Examines decsions on kick offs (surprise on sides or regular kick off AND accept touchback or return kick) using an expected points frame work.  Shows that teams are moderately risk and loss averse. -->
<!-- https://www.eng.buffalo.edu/~jzhuang/Papers/JQAS_NFL_2011.pdf -->







## In sports with continuous states

Even in sports where the concept of a state is not so easy to define, the value of a possession can be estimated with the help of tracking data. 
Over the past decade or so, professional sports leagues have collected tracking data which record the locations of all players and the ball (or puck) throughout a game. 
This high-resolution data allows researchers to produce advanced analyses of the captured spatio-temporal information and better understand the game. 
This is a great leap forward from older resources such as traditional box-score results and play-by-play data.

In basketball, @cervone2014pointwise and @cervone2016multiresolution introduce expected possession value (EPV) as a means toward an assessment of a player's on-court performance. 
This metric is a continuous-time estimate of the expected number of points for the offensive team on a given possession using player and ball locations. 
The EPV takes into account all possible outcomes (a shot attempt, a pass, etc.) for a given player with the ball, with different weights being assigned to each decision. 
The computation of the EPV statistic is done using a Markov model.
Consequently, the authors derive a metric called EPV-Added (EPVA), measuring a player's EPV contribution in a given situation relative to a league-average player.

A demonstration of the EPV model presented in @cervone2016multiresolution is available at <https://github.com/dcervone/EPVDemo>. 
Figure \ref{fig:epv-demo} illustrates how the provided tracking data informs the evolution of EPV throughout the play. 
It displays a snapshot of a possession during the NBA regular season matchup between the Miami Heat and the Brooklyn Nets on November 1, 2013. 
Miami is the team on offense in this possession, whose outcome is a 26-foot three-point miss by Mario Chalmers. 
The plot consists of two elements: 
1) (bottom) the player locations on the court at a particular moment in this possession: when the ball just left Chalmers' hands, 
and 2) (top) a line graph showing how the EPV changes continuously throughout the play until the three-point attempt. 
For this possession, the estimated EPV for the Miami Heat reaches its peak at $1.276$ points at the moment the shot is taken.

```{r epv-demo, out.width="100%", fig.cap="Player locations and estimated EPV for a possession during the Miami Heat (red) vs. Brooklyn Nets (black) NBA game on November 1, 2013. The captured moment is when Miami's Mario Chalmers just releases a three-point shot, which ends up as a missed field goal."}
# Source: https://github.com/dcervone/EPVDemo/blob/master/gifs/poss_12.gif
# library(magick)
# epv <- image_read("https://github.com/dcervone/EPVDemo/blob/master/gifs/poss_12.gif?raw=true")
# image_write(epv[529], "epv_snap.png")
knitr::include_graphics("epv_snap.png")
```

Note that Miami starts the play with an EPV of approximately 1.0 points.
This indicates their implied average points per possession. 
Chalmers's shot is worth 3 points, so the EPV of 1.276 points implies that the model estimate of the probability of Chalmers making this shot is 42.5\%. 
A breakthrough in this work is that this estimate is conditional on the locations of the other 9 players on the basketball court!

Another framework for estimating expected points in basketball is proposed by @Sicilia2019DeepHoops. 
The authors offer a different point of view on expected points, where they first consider a classification model which returns the probabilities for whether a player would commit a foul (shooting and non-shooting), turnover, or attempt a shot. 
The values associated with each of those "terminal actions" are then used to compute the expected points within a basketball play.

See also @bornn2017studying for more information on how tracking data have enabled advanced statistical analyses of basketball in recent years.

In American football, @Yurko2020going use tracking data provided by the [2019 NFL Big Data Bowl](https://operations.nfl.com/gameday/analytics/big-data-bowl/past-big-data-bowl-recaps/) to model the expected yards gained for a ball-carrier during the course of a play. 
As an extension to pre-existing approaches, the authors use conditional density estimation to obtain a probability distribution for the number of yards gained during the play, rather than only producing a single estimate for the expected yards gained. 
Accordingly, the probability of various types of outcomes at the end of a play such as a touchdown or a first-down gain can be computed from the distribution of the end-of-play yard line.

The notion of EPV has also been extended to association football (soccer.)
@fernandez2021framework implement deep learning methods to examine the instantaneous expected value of soccer possessions. 
This approach considers passes, ball drives, and shots in soccer as the main set of actions used to compute expected possession value. 
Many applications can be derived from this framework, including predicting which footballer on the pitch is most likely to receive the next pass from the current on-ball player.

## Optimal strategies that don't maximize expected points

In Section \ref{sec:ev}, we defined the expected value of a possession based on the state $s$ of the game in terms of the expected number of points (runs) $X$ that would be scored in the remainder of some period of time. 
We then showed how this value could be used to analyze the relative effectiveness of certain strategies, with the simple idea that strategies that yield higher expected values are preferable. 
Generally, the goal of any sport is to score more points than the other team, which most often means trying to score as many points as possible, leading to a general strategy of maximizing expected points. 
However, there are situations in which maximizing the number of expected points is *not* the desired strategy. 

For example, in the bottom of the ninth inning of a tied baseball game, the optimal strategy for winning the game is maximizing the probability of scoring *at least one run*, which may differ from the strategy of maximizing expected runs. 
If we let $U$ be the set of all strategies, then we assert that it is not always the case that the strategy $u$ that maximizes the expected number of points will maximize the probability of winning:
$$
  \underset{u \in U}{\arg \max \,}{\Pr[X > 0 | s, u]} \neq \underset{u \in U}{\arg \max \,}{\mathbb{E}[X | s, u]} \,.
$$
Consider the situation where runners are on first and third base, and the score is tied in the bottom of the ninth inning with no one out. 
Table \ref{tab:tied} reveals that the expected number of runs scored in the remainder of the inning is 1.94 runs, while the the probability of scoring zero runs is 0.13. 
The defense is in a tight spot, facing an 87% probability of losing the game. 
However, by walking the hitter to load the bases, they create the opportunity to force the lead runner at home and thus reduce the chances of scoring to 82%, even though they raise the expected number of runs scored to 2.22. 
In this case, the defensive team is wise to pursue the strategy of maximizing the expected number of runs scored, because it *minimizes* the probability of scoring at least one run. 

```{r}
erm <- erm |>
  mutate(
    Pr_0 = c(
      .747, .855, .933,
      .604, .734, .886, 
      .381, .610, .788,
      .120, .307, .738,
      .395, .571, .791,
      .130, .367, .717,
      .180, .270, .668,
      .180, .303, .671
    )
  )

erm |>
  filter(Out == 0, Base %in% c(5, 7)) |>
  mutate(Base = int2bin(Base)) |>
  kable(booktabs = TRUE, 
        caption = "Loading the bases can be a rational strategy. In this case, loading the bases maximizes both the expected number of runs scored *and* the probability of not scoring at all. \\label{tab:tied}")
```

Maximizing the probability of scoring is optimal in any sudden-death situation, which has (but currently does not) included overtime in American football [@martin2018markov]. 

The situation gets even more interesting when teams modify both their offensive and defensive strategies simultaneously. 
For example, in hockey teams will often pull their goalie when trailing in the final period. 
This strategy severely weakens their defense, but strengthens their offense. 
The hope is to score a quick goal to get back in the game, but the risk is falling further behind. 
@beaudoin2010strategies show that NHL coaches do not always employ the optimal strategies. 
@skinner2011scoring develops a general framework for these desperation strategies, which include the onside kick in American football, pulling the infield and/or outfield in in baseball, and of course, the fabled Hack-a-Shaq strategy in basketball. 


# Win probability {#sec:wp}

A related, but different concept is the notion of *win probability*.
Win probability is simply an estimate of the probability that a team will win the game, given its current state $s$. 
Extending the mathematical framework we defined in Section \ref{sec:ev}, let $W_i$ be a binary random variable that indicates a win for team $i$. 
Then, 
$$
  \Pr[W_i | s] \,,
$$
is the win probability for team $i$ in the state $s$. 

This win probability is closely related to the expected value of a state. 
@Albert2015 defines the win probability as:
$$
  \Pr[W_i | s] = \sum_{X \geq 0} \Pr[X | s] \cdot \Pr[W_i | X, s] \,,
$$
where $\Pr[W_i | X, s]$ is the probability that team $i$ will win the game given that they score $X$ points from state $s$. 

Win probability is easily extended to provide a measure of the impact of sports plays and individual player contributions, as discussed in @Albert2015. 
Given its popularity, recent books on sports analytics often dedicate multiple chapters entirely to win probability. 
These include @Albert2001curve, @Schwarz2005numbers, @Tango2007book, @Albert2017handbook, and @Winston2022. 

In this section, we discuss notable previous work on win probability in baseball, American football, basketball, and several other sports. 

## Baseball

The notion of win probability in baseball goes back to at least as early as @Lindsey1961.
In this paper, the author calculates the expected win probability after each inning based on the distribution of runs scored in each inning. 
Inspired by @Lindsey1961's work, @Mills1970 utilize win probability to introduce Player Win Average (PWA), a measure of a player's contribution to the game outcome. 
In particular, PWA is computed as 
$$
PWA = \frac{Win \ Points}{Win \ Points + Loss \ Points},
$$
where the win and loss points represent how much the player positively and negatively impact their team's probability of winning after each play.
In effect, the win points are the sum of the changes in $\Pr[W_i | s]$ from one state to the next.

Additionally, a mathematical model for estimating win probability in baseball is presented in @Tango2007book. 
The authors use Markov chains to look at win expectancy throughout the course of a baseball game. 
This approach considers different states of the game such as base, inning, outs and score, and outputs win probabilities accordingly.

See @Albert2015 for a more detailed historical overview of the use of win probability in baseball. 

## American football

In recent years, a number of statistical methods have been used to build well-calibrated win probability models in American football. 

These are flexible models that have high predictability, can account for nonlinear interactions between the explanatory variables, require few assumptions, and produce feature importance scores.

@Lock2014 use a random forest method to provide a win probability estimate before each play in a football game. 
Covariates included in this tree-based win probability model are the current down, score differential, time remaining, adjusted score, point spread, number of timeouts remaining for each team, total points scored, current yard line, and yards to go for a first down. 
According to this model, the difference in score between the two teams is the most important feature for predicting win probabilities at any given moment in an NFL game.

In addition, @Yurko2019nflwar estimate win probability in the NFL using a generalized additive model (GAM), as part of the **nflscrapR** package [@R-nflscrapR] and nflWAR framework. 
This model takes into account the estimated expected points obtained from the model described in Section \ref{sec:ev}, along with other predictors for time, current half, and timeouts. 
The two win probability frameworks proposed by @Lock2014 and @Yurko2019nflwar were also implemented in @Yam2019 with minimal modifications. Specifically, the authors combined both approaches to estimate the win probability for each play, with an overall goal of assessing fourth down decision-making in gridiron football.

A vital highlight of @Yurko2019nflwar's win probability model is that it is fully reproducible and uses publicly available data. 
One of @Yurko2019nflwar's goals was also to encourage researchers to "use, explore, and improve upon our work," which ultimately inspired \pkg{nflfastR} (@R-nflfastR), now considered the successor to **nflscrapR**. 

Figure \ref{fig:nflfastr-wp} shows a win probability graph for the January 14, 2018 NFL Playoffs Divisional Round matchup between the New Orleans Saints and the Minnesota Vikings. 
We obtain the estimated probability of winning for each team using the \pkg{nflfastR} R package, which implements a gradient boosting model via the \pkg{xgboost} library (@R-xgboost) for estimating win probabilities. 
Minnesota was leading throughout the first three quarters of the game, having win probabilities of 0.869, 0.941, and 0.742 at the end of the first, second, and third quarters, respectively. 
The win probabilities get close to parity late in the fourth quarter, when the Saints took the lead with 3:01 left in the game. 
The last play of this game---famously known as the [Minneapolis Miracle](https://en.wikipedia.org/wiki/Minneapolis_Miracle)---resulted in a drastic swing in win probabilities for both teams. 
With 10 seconds remaining in the game, the Vikings begin the final possession with a 25.3% chance of winning. 
Their probability increased to a perfect 1 when Stefon Diggs scored a game-winning 61-yard receiving touchdown as the game clock expired.

```{r}
# win prob for Minneapolis Miracle
library(teamcolors)

miracle <- teamcolors |> 
  filter(league == "nfl", grepl("Vikings|Saints", name)) |>
  arrange()
```

```{r nflfastr-wp, fig.cap="Win probability graph for New Orleans Saints vs. Minnesota Vikings in the 2017--18 NFL Playoffs."}
library(nflfastR)
x <- load_pbp(2017) |> 
  filter(
    season_type == "POST",
    home_team == "MIN",
    away_team == "NO",
    !is.na(home_wp),
    !is.na(away_wp)
  ) |>
  select(game_seconds_remaining, home_wp, away_wp) |> 
  pivot_longer(
    !game_seconds_remaining,
    names_to = "team",
    values_to = "wp"
  )

wp_plot <- ggplot(x, aes(game_seconds_remaining, wp, color = team)) +
  scale_color_manual(
    values = setNames(miracle$primary, c("home_wp", "away_wp")), 
    labels = miracle$name
  ) +
  geom_hline(yintercept = 0.5, color = "gray", linetype = "dashed") +
  geom_vline(
    xintercept = seq(0, 3600, 900), 
    linetype = "dashed", 
    color = "gray"
  ) + 
  geom_line(size = 1.5, alpha = 0.8) +
  labs(
    x = "Time Remaining (seconds)",
    y = "Win Probability",
    color = NULL
  ) +
  theme_bw() +
  theme(legend.position = "top")

library(magick)
logos <- miracle |>
  pull(logo) |>
  map(image_read) |>
  map(as.raster)

wp_plot + 
  annotation_raster(logos[[1]], -1010, -1490, 0.76, 0.865) + 
  annotation_raster(logos[[2]], -1010, -1490, 0.135, 0.24) + 
  scale_x_reverse(breaks = seq(0, 3600, 900))
```


## Basketball

@Stern1994 provides an investigation of in-game win probability and the scoring process in basketball using a Brownian motion model. 
Let $p(l, t)$ represent the win probability for the home team given an $l$-point lead after $t$ seconds of game time. 
The model introduced by @Stern1994 is a probit regression model, which provides an estimate for $p(l, t)$. 
Specifically, 
$$
p(l, t) = \Phi\left(\frac{l + (1-t)\mu}{\sqrt{(1-t)\sigma^2}}\right)
\,.
$$
Here, a Brownian motion process with drift $\mu$ points advantage for the home team and variance $\sigma^2$ is used to model the score difference between the home and away teams. 

On a related note, @Deshpande2016 extend @Stern1994's framework by applying it in a Bayesian setting. 
In this paper, the authors propose a Bayesian linear regression model to assess the impact of individual players on their team's chance of winning at any given time of a basketball game. 

This model assumes independence of observations and constant variability in win probability.

In addition, @McFarlane2019 uses logistic regression to estimate win probability for evaluating end-of-game decisions in the NBA. 
The approach takes into account the remaining game time, score difference, and point spread. 
This win probability model is then applied to the calculation of the End-of-game Tactics Metric (ETM), measuring how the chance of winning a basketball game differs between the optimal and on-court actual decisions.

## Other sports

The idea of win probability is also applied to other sports, with a diverse range of statistical techniques being used to estimate the probability of winning for a player or team. 
@Brenzel2019 use three-dimensional Markov models to estimate win probability throughout a curling match. 
In particular, the authors propose both homogeneous and heterogeneous Markov models for estimating the chance of winning in curling, with different independence assumptions on the relationship between performance and the current state of the game. 
In esports, @maymin2021smart uses logistic regression to build a well-calibrated in-game win probability model for each specific moment during a game of League of Legends. 
Moreover, @Guan2022 develop an in-game win probability model for the National Rugby League using functional data analysis. 
In this approach, the rugby event data are treated as functional, and the win probability is expressed as a function of the match time. 

# Team strength {#sec:strength}

A third crucial idea in sports analytics is the estimation of team strength. 
A popular method for estimating team strength in sports is through pairwise evaluations. 
Perhaps the most famous probability model for predicting the outcome of a paired comparison is the Bradley-Terry model (BTM) [@Bradley1952]. 
For a pair of players (or teams) $i$ and $j$, let $\Pi_{ij}$ denote the probability that $i$ is preferred to $j$. 
Then the BTM is a logistic regression model with parameters $\beta_i, \beta_j$ such that
$$
\log\left(\frac{\Pi_{ij}}{\Pi_{ji}}\right) = \beta_i - \beta_j \,.
$$
Here, $\exp{\beta_i}$ is often viewed as a representation of player $i$'s ability. 

The BTM can be implemented in `R` [@R-base] via the \pkg{BradleyTerry2} package [@R-BradleyTerry2]. 
As an example, we consider the data given in @agresti2018introduction (page 247) on tennis results from 2014--2018 for five men's professional players: Novak Djokovic, Roger Federer, Andy Murray, Rafael Nadal, and Stan Wawrinka. 
We fit a BTM to estimate the win probability for each pair of players and obtain a ranking for this group of five.

```{r}
tennis <- dplyr::tibble(
  Djokovic = c(NA, 9, 14, 9, 4),
  Federer = c(6, NA, 5, 5, 7),
  Murray = c(3, 0, NA, 2, 2),
  Nadal = c(2, 1, 4, NA, 4),
  Wawrinka = c(3, 2, 2, 3, NA)
) |>
  t() |>
  BradleyTerry2::countsToBinomial()

tennis_bt <- BradleyTerry2::BTm(cbind(win1, win2), player1, player2, data = tennis)

tennis_bt |>
  update(refcat = "Wawrinka") |> 
  BradleyTerry2::BTabilities() |> 
  round(3) |> 
  kable(booktabs = TRUE, 
        col.names = c("Ability", "SE"),
        caption = "The estimated abilities (with standard errors) for each tennis player, relative to Wawrinka, obtained from the fitted Bradley-Terry model. \\label{tab:tennisbtm}") |> 
  kable_styling(latex_options = "hold_position")
```

Table \ref{tab:tennisbtm} shows the estimated coefficients of the fitted BTM. 
According to the abilities, between 2014 and 2018 the players are ranked as follows: 1) Djokovic, 2) Federer, 3) Wawrinka, 4) Nadal, 5) Murray.
To obtain win probabilities, as an illustration, for the Federer-Nadal matchup, an estimate for the probability of a Federer victory is
$$
\hat\Pi_{24} = \frac{exp(\hat\beta_2 - \hat\beta_4)}{1 + exp(\hat\beta_2 - \hat\beta_4)} = \frac{exp(1.136 + 0.062)}{1 + exp(1.136 + 0.062)} = 0.768.
$$

```{r btm, eval=FALSE}
library(BradleyTerry2)
# show BTM for Pirates vs. Yankees in 1960
# or Red Sox vs. Yankees in 2003/2004
library(retrosheet)
games <- get_retrosheet("game", 2004)

bt_games <- games |>
  group_by(HmTm, VisTm) |>
  summarize(
    home_w = sum(HmRuns > VisRuns),
    away_w = sum(VisRuns > HmRuns),
  ) |>
  mutate(
    HmTm = factor(HmTm),
    VisTm = factor(VisTm, levels = levels(HmTm))
  )

btm <- BTm(
  outcome = cbind(home_w, away_w), 
  player1 = HmTm, 
  player2 = VisTm,
  data = bt_games, 
  id = "team"
)
btm

bt_games2 <- bt_games |>
  mutate(
    HmTm = data.frame(team = HmTm, at_home = 1),
    VisTm = data.frame(team = VisTm, at_home = 0)
  )

btm2 <- btm |>
  update(formula = ~ team + at_home, data = bt_games2)
btm2

btm |>
  broom::tidy() |>
  arrange(desc(estimate))
btm2 |>
  broom::tidy() |>
  arrange(desc(estimate))
```

Another widely known tool for measuring team strength is the Elo rating system [@Elo1978]. The Elo rating was originally developed for chess, but also has been used to estimate team strength in other sports. 
See @koning2017rating for more information on applications of the Elo rating in soccer. 

@Glickman1998 propose a Bayesian state-space model for paired comparisons for predicting NFL games, allowing team strength parameters to vary over time. 
In particular, they model point differential in the NFL by introducing week-to-week and season-to-season as the two primary sources variation in team strengths. 
See also @glickman2017estimating for more discussion on estimating team strengths in American football. 
More recently, @Lopez2018 extend @Glickman1998's state-space model to understand randomness in the four major American sports leagues. 
Betting moneylines are used in place of point differentials in order to estimate team strengths, and this framework also accounts for home advantage.

# Tools {#sec:tools}

Analytical work in sports requires facility with an ever-changing set of computational tools for working with data. 
Sources of authoritative data about sports are myriad, and are too numerous to list here. 
Software tools for sports analytics are similarly numerous. 
For R, we maintain a [CRAN Task View for Sports Analytics](https://CRAN.R-project.org/view=SportsAnalytics) that catalogs R packages published on the Comprehensive R Archive Network and organizes them by sport [@baumer2022ctv].
A more general collection of software tools is being curated by the SportsDataverse initiative [@gilani2022sportsdataverse].

In the remainder of this section, we highlight a few tools for sports analytics that are of general interest and illustrate a common paradigm for how these tools can be used in conjunction. 

## Case study in how tools fit together: chess

Many tools in sports analytics provide the ability to read, write, and plot data stored in a sport-specific format. 
For example, consider chess, where the sequence of moves in games is often recorded in [Portable Game Notation](https://en.wikipedia.org/wiki/Portable_Game_Notation).
Software tools can then be built around this well-defined format. 
The \pkg{chess} package [@R-chess] provides R users with the ability to read, write, display, and manipulate chess data in PGN format. 
[Application programming interfaces](https://en.wikipedia.org/wiki/API) (APIs) are also common. 
In chess, the \pkg{chessR} package [@R-chessR] allows R users to download game data from the Chess.com API. 
This type of infrastructure, where one package is the "workhorse" that facilitates common generic data operations, and other packages layer on specific functionality, is common in sports analytics. 

Figure \ref{fig:chess} shows a rendering of the starting chess board obtained via the \pkg{chess} package, along with the final position in a game recently won by one of the authors (downloaded via the \pkg{chessR} package). 

```{r chess, eval=whoami::username() == 'bbadumer', fig.show="hold", out.width="49%", fig.cap="At left, the starting chess board printed via the \\pkg{chess} package. At right, the final position for one of the authors' recent wins (a checkmate playing Black)."}
library(chess)
# Sys.setenv(RETICULATE_PYTHON = "my_env/bin/python3")
game() |> plot()

library(chessR)
us <- c("fatgreggy")
greg <- get_raw_chessdotcom(
  usernames = us,
  year_month = 202206
)

m <- extract_moves_as_game(greg[6, ])
# plot_moves(m, interactive = FALSE)
plot(m)
```

## Colors

Each professional sports team has its own brand, most obviously identified by a team logo and set of colors. 
The \pkg{teamcolors} package [@R-teamcolors] provides color palettes and logos for men's and women's professional and collegiate sports teams, as well as color and fill scale functions compatible with \pkg{ggplot2} [@R-ggplot2].
The team colors and logos shown in Figure \ref{fig:nflfastr-wp} were provided by the \pkg{teamcolors} package. 
Figure \ref{fig:teamcolors} illustrates the full array of team color palettes for professional sports leagues. 

```{r teamcolors, warning=FALSE, fig.cap="Professional sports team color palettes provided by the \\pkg{teamcolors} package. ", fig.height=9}
teamcolors::show_team_col()
```

# Opportunities

## Competitions

- Kaggle March Madness [@lopez2015building] (jqas special issue: https://www.degruyter.com/journal/key/jqas/11/1/html)
- Big Data Bowl (special issue https://www.degruyter.com/journal/key/jqas/16/2/html)
- Big Data Cup
- SABR Case competition

## Conferences

- NESSIS/CASSIS
- Sloan
- CMU
- UConn sports analytics

@marchi2018analyzing

@R-Lahman, @R-pitchRx, @R-retrosheet


# Conclusion

<!--
Sum up the key conclusions of your review, highlighting the most promising scientific developments, directions for future research, applications, etc. The conclusion should be ~2 paragraphs, ~750 words total.
-->

As an applied science, sports analytics may lack a [grand unified theory](https://en.wikipedia.org/wiki/Grand_Unified_Theory) that succinctly characterizes game play across all sports. 
However, as a maturing discipline, sports analytics has been able to address fundamental questions common to many sports. 
In this paper, we explicate three of those big questions: 

- Who are the best teams and how good are they? 
- What is the likelihood of each team winning the game at any given juncture? 
- Is there a generic framework for evaluating strategies at any given juncture in a game? 

Other fundamental questions, such as:

- How significant is the element of chance in a particular sport? 
- Given that we know how the best teams are, who are the best players and how can we quantify their relative contributions? 
- What combinations of players work best together in a particular sport? 

are addressed elsewhere. 
In particular, see @Lopez2018 for estimations of the element of chance across four major sports. 
The second question is often addressed using a formulation of *wins above replacement* (WAR)---see @baumer2015openwar and @Yurko2019nflwar for details in baseball and American football. 
The third question is most compelling in sports like basketball, ice hockey, and soccer, where substitutions are common and it is obvious that different combinations of players with different sets of skills will result in squad of varying strengths and weaknesses.
The concept of *plus-minus*, and then *adjusted* plus-minus is frequently applied to address this question (see @hvattum2019comprehensive for a comprehensive overview of applications). 

In drawing together these three big ideas in sports analytics, we have also drawn attention to some computational tools for doing the work and opportunities to showcase that work. 
It is through these exchanges of ideas, tools, models, and data that analytics moves our collective understanding of sports forward.  

# Acknowledgments

We are grateful to Michael Lopez and Katherine Evans for their thoughts on early versions of this paper. 

The R Markdown file that generated this paper, including all R code, is available at <https://github.com/beanumber/wire21>.

# References {-}
